---
title: "03-ARIMA-basics"
author: "Boyko Amarov"
date: "11/2/2021"
output: html_document
---

# ARIMA

Time series process: sequence of random variables

$$
y_1, y_2,\ldots,y_{T}
$$

AR(1)
$$
y_{t} = \delta + \alpha y_{t - 1} + e_t
$$

Statistical properties:

1. Expected value
$$
E(y_t) = ?
$$
2. Variance

Expected squared deviation from the expected value.

$$
Var(y_t) := E[(y_{t} - E(y_t))^2]
$$


3. Autocovariances/autocorrelations of time series

$$
Cov(y_{t}, y_{t - 1}) := E\left(y_{t} - E(y_{t})\right)(y_{t - 1} - E(y_{t - 1}))
$$

$$
Cov(y_t, y_{t - 1}) \text{ 1st order autocovariance}\\
Cov(y_t, y_{t - 2}) \text{ 2nd order autocovariance}\\
\vdots\\
Cov(y_t, y_{t - k}) \text{ k-th order autocovariance}\\
\rho(y_{t}, y_{t - 1}) = \frac{Cov(y_t, y_{t - 1})}{Var(y_t)} \text{ 1st order autocorrelation}\\
\rho(y_{t}, y_{t - 2}) = \frac{Cov(y_t, y_{t - 2})}{Var(y_t)} \text{ 2st order autocorrelation}\\
\vdots\\
\rho(y_{t}, y_{t - k}) = \frac{Cov(y_t, y_{t - k})}{Var(y_t)} \text{ k-th order autocorrelation}\\
$$
The autocorrelations do not depend on the unit of measurement of the series and 
are all (unitless) numbers between -1 and 1:


$$
-1 \leq \rho(y_{t}, y_{t - k}) \leq 1.
$$
Autocorrelation of 1 between e.g. $y_{t}$ and $y_{t - 1}$ means that $y_{t}$ is a
linear function of $y_{t - 1}$ (with a positive slope coefficient).

Autocorrelation of -1 between e.g. $y_{t}$ and $y_{t - 1}$ means that $y_{t}$ is a
linear function of $y_{t - 1}$ (with a negative slope coefficient).


```{r}
## For illustration only

n <- 100

sim_data <- expand_grid(
  mu = c(0, 10),
  sigma = c(1, 3),
  t = 1:100
) %>%
  mutate(
    y = rnorm(n(), mean = mu, sd = sigma),
    sigma_lab = paste0("sigma = ", sigma),
    mu_lab = paste0("mu = ", mu)
  )

sim_data %>%
  ggplot(aes(x = t, y = y, color = mu_lab)) +
  geom_point(size = 1 / 2) +
  geom_line() +
  facet_wrap(~sigma_lab) +
  labs(
    x = "t",
    y = expression(y[t])
  )
```
```{r}
sim_data %>%
  group_by(mu_lab, sigma_lab) %>%
  summarise(
    Average = mean(y),
    StdDev = sd(y)
  )
```

Covariance

$$
\begin{aligned}
  x_{t} & = u_{t}\\
  y_{t} & = -2 x_{t} + e_{t}
\end{aligned}
$$


```{r}
n <- 100
x <- rnorm(n = n, mean = 0, sd = 1)
y <- 2 * x + rnorm(n = n, mean = 0, sd = 1)

tibble(y, x) %>%
  ggplot(aes(x = x, y = y)) +
    geom_point()

cov(x, y)
cor(x, y)
```


## AR(1)

Autoregressive, because the model equation contains lags of the time
series $y_{t-1}, y_{t - 2}, \ldots, y_{t - k}$.

$$
y_{t} = 2 + 0.5y_{t - 1} + e_{t}
$$

$e_t$ is a purely random process (white noise process).

$$
E(e_t) = 0, \text{ for every } t\\
Var(e_t) = \sigma^2 \text{ for every }  t\\
Cov(e_t, e_{t - k}) = 0, \text{ for every } k \neq 0
$$
$$
e_{t} \sim WN(\sigma^2)
$$
Expected value of the AR(1) process

$$
y_{t} = 2 + 0.5y_{t - 1} + e_{t}, \quad e_{t} \sim WN(\sigma^2)
$$

$$
\begin{aligned}
  E(y_t) & = E(2 + 0.5y_{t - 1} + e_{t}) \text{ because of linearity}\\
         & = E(2) + E(0.5y_{t - 1}) + E(e_t) \\
         & = 2 + 0.5 E(y_{t - 1}) + 0 \\
  E(y_t) & = 2 + 0.5 E(y_{t - 1})\\
\end{aligned}
$$
Let us assume that the expected value of the process does not change over
time (does not depend on the time index $t$). In other words we assume that
the process is **mean stationary** (expected value does not change over time).

$$
E(y_{t}) = E(y_{t - 1}) = \mu
$$
Then the equation for the expected values is:


$$
\begin{aligned}
  \mu & = 2 + 0.5 \mu\\
  (1 - 0.5) \mu & = 2 \implies\\
  \mu & = \frac{2}{1 - 0.5} = 4
\end{aligned}
$$


$$
y_{t} = \delta + \alpha y_{t - 1} + e_t, \quad e_t \sim WN(\sigma^2)
$$

$$
E(y_{t}) = \frac{\delta}{1 - \alpha}
$$
New: lag-operator
$$
Ly_{t} = y_{t - 1}
$$

$$
\begin{aligned}
y_{t} & = \delta + \alpha y_{t - 1} + e_t\\
y_{t} & = \delta + \alpha Ly_{t} + e_t\\
(1 - \alpha L) y_{t} & = \delta + e_t\\
y_{t} & = \frac{\delta}{1 - \alpha L} + \frac{e_t}{1 - \alpha L}\\
\end{aligned}
$$
$$
S_{n} = 1 + \alpha + \alpha^2 + \alpha^3 + \ldots + \alpha^n\\
\lim_{n \to \infty} S_{n} = \frac{1}{1 - \alpha} \text{ for } |\alpha| < 1
$$
```{r}
n <- 60
alpha <- 0.5
sum((alpha)^(0:n))
```

$$
\text{ For } |\alpha| < 1\\
\begin{aligned}
y_{t} & = \frac{\delta}{1 - \alpha L} + \frac{e_t}{1 - \alpha L}\\
y_{t} & = \delta (1 + \alpha L + \alpha^2L^2 + \ldots) + e_{t}(1 + \alpha L + \alpha^2L^2 + \ldots) \\
y_{t} & = \delta (1 + \alpha + \alpha^2 + \ldots) + e_{t}(1 + \alpha L + \alpha^2L^2 + \ldots) \\
y_{t} & = \frac{\delta}{1 - \alpha} + e_{t}(1 + \alpha L + \alpha^2L^2 + \ldots) \\
y_{t} & = \frac{\delta}{1 - \alpha} + (e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots) \\
\end{aligned}
$$
$$
\begin{aligned}
y_{t} & = \frac{\delta}{1 - \alpha} + (\alpha^{0}e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots) \\
y_{t} & = \frac{\delta}{1 - \alpha} + \sum_{k = 0}^{\infty} \alpha^{k} e_{t - k} \\
\end{aligned}
$$

<!-- \alpha^{k}e_t \text{ for } k = 1 \implies \alpha^{1}e_{t - 1} -->

$$
\begin{aligned}
E(y_{t}) & = E\left(\frac{\delta}{1 - \alpha} + \sum_{k = 0}^{\infty} \alpha^{k} e_{t - k}\right) \\
E(y_{t}) & = E\left(\frac{\delta}{1 - \alpha}\right) + E\left(\sum_{k = 0}^{\infty} \alpha^{k} e_{t - k}\right) \\
E(y_{t}) & = \frac{\delta}{1 - \alpha} + \sum_{k = 0}^{\infty} \alpha^{k} E(e_{t - k}) \\
E(y_{t}) & = \frac{\delta}{1 - \alpha}\\
\end{aligned}
$$
$$
\begin{aligned}
Var(y_t) & = E\left((y_{t} - E(y_{t}))^2\right)\\
Var(y_t) & = E\left((\frac{\delta}{1 - \alpha} + \sum_{k = 0}^{\infty} \alpha^{k} e_{t - k} - \frac{\delta}{1 - \alpha})^2\right)\\
Var(y_t) & = E\left(\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)^2\right)\\
Var(y_t) & = E\left(\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)\right)\\
\end{aligned}
Var(y_t) & = E\left(\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)\right)\\
\end{aligned}
$$

$$
Var(y_t) = E\left(
    \begin{array}{c}
    e_t\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right) +\\
    \alpha e_{t - 1}\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right) + \\
    \alpha^2 e_{t - t}\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)
    + \\
    \vdots
    \end{array}
  \right)
$$

$$
Var(y_t) = E\left(
    \begin{array}{c}
    \left( e_{t}^2 + \alpha e_te_{t - 1} + \alpha^2e_te_{t - 2} + \ldots \right) +\\
    \left( \alpha e_{t}e_{t - 1} + \alpha ^2 e_{t - 1} e_{t - 1} + \alpha^3e_te_{t - 2} + \ldots \right) + \\
    \alpha^2 e_{t - t}\left( e_{t} + \alpha e_{t - 1} + \alpha^2e_{t - 2} + \ldots \right)
    + \\
    \vdots
    \end{array}
  \right)
$$
$$
E(e_t^2) = E((e_t - E(e_t))^2) = E(e_t^2) = \sigma^2
$$
Definition of covariance
$$
Cov(e_{t}, e_{t - 1}) = E((e_t - E(e_t))(e_{t - 1} - E(e_t)))\\
 = E(e_te_{t - 1})
$$
Properties of the lag operator
$$
Ly_{t} = y_{t - 1}\\
L^2y_{t} = L(Ly_{t}) = L(y_{t - 1}) = y_{t - 2} \\
L^{k}y_{t} = y_{t - k}\\
L\delta := \delta, \quad \delta \text{ is a constant} \\
$$
